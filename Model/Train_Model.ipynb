{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e500c4c4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('/kaggle/working/install_done.txt'):\n",
    "    print(\"--- Lần đầu chạy, đang cài đặt các thư viện cần thiết...\")\n",
    "    !pip install --quiet \\\n",
    "        \"torch==2.3.1\" \"torchvision==0.18.1\" \"torchaudio==2.3.1\" \\\n",
    "        \"lightning[pytorch-extra]==2.2.5\" \\\n",
    "        \"scikit-learn==1.4.2\" \\\n",
    "        \"torchmetrics==0.11.4\" \\\n",
    "        \"transformers==4.42.3\"\n",
    "    with open('/kaggle/working/install_done.txt', 'w') as f:\n",
    "        f.write('OK')\n",
    "    print(\"--- Cài đặt hoàn tất.\")\n",
    "else:\n",
    "    print(\"--- Thư viện đã được cài đặt. Bắt đầu chạy chương trình chính...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435c89e0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json, itertools, torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Linear, CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, jaccard_score, accuracy_score, hamming_loss\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "ENTITY_CLASSES = [\"kol\", \"product\"]\n",
    "ATTRIBUTE_CLASSES = [\"color\", \"performance\", \"packaging\", \"texture\", \"price\", \"ingredients\", \"personality\", \"appearance\", \"skill\", \"authenticity\", \"brand_collaboration\", \"null\"]\n",
    "SENTIMENT_CLASSES = [\"positive\", \"negative\", \"neutral\"]\n",
    "_classes = list(itertools.product(ENTITY_CLASSES, ATTRIBUTE_CLASSES, SENTIMENT_CLASSES))\n",
    "ID2LABEL = {index: \"#\".join(_class) for index, _class in enumerate(_classes)}\n",
    "LABEL2ID = {v: k for k, v in ID2LABEL.items()}\n",
    "NUM_CLASSES = len(ID2LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dad78f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class XLMRobertaCommentClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes: int, num_predictions: int = 5, lr: float = 2e-5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lm = AutoModel.from_pretrained(\"uitnlp/CafeBERT\")\n",
    "        self.cls = Linear(1024, self.hparams.num_classes * self.hparams.num_predictions)\n",
    "        self.criterion = CrossEntropyLoss(ignore_index=-1)\n",
    "        self.val_f1 = MulticlassF1Score(num_classes=self.hparams.num_classes, average='micro', ignore_index=-1)\n",
    "        self.test_f1 = MulticlassF1Score(num_classes=self.hparams.num_classes, average='micro', ignore_index=-1)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.lm(input_ids=input_ids, attention_mask=attention_mask)[\"last_hidden_state\"][:, 0, :]\n",
    "        x = self.cls(x)\n",
    "        return x\n",
    "\n",
    "    def _process_batch(self, batch):\n",
    "        logits = self.forward(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "        reshaped_logits = logits.view(-1, self.hparams.num_classes)\n",
    "        reshaped_labels = batch[\"labels\"].view(-1)\n",
    "        loss = self.criterion(reshaped_logits, reshaped_labels)\n",
    "        return loss, reshaped_logits, reshaped_labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx): \n",
    "        loss, _, _ = self._process_batch(batch)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx): \n",
    "        loss, logits, labels = self._process_batch(batch)\n",
    "        self.val_f1(logits, labels)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_f1\", self.val_f1, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, logits, labels = self._process_batch(batch)\n",
    "        self.test_f1(logits, labels)\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_f1\", self.test_f1, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=self.hparams.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4781c81b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, data_path: str, tokenizer: AutoTokenizer, label2id: dict, null_label_id: int, num_predictions: int = 5, max_len: int = 256):\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.data = json.load(f)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.null_label_id = null_label_id\n",
    "        self.num_predictions = num_predictions\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        item = self.data[index]\n",
    "        text = item[\"text\"]\n",
    "        label_ids = [self.label2id[label[-1].lower()] for label in item[\"labels\"]]\n",
    "        if len(label_ids) < self.num_predictions:\n",
    "            label_ids.extend([self.null_label_id] * (self.num_predictions - len(label_ids)))\n",
    "        else:\n",
    "            label_ids = label_ids[:self.num_predictions]\n",
    "\n",
    "        encoding = self.tokenizer(text, add_special_tokens=True, max_length=self.max_len, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label_ids, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8727bcad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def main_pipeline():\n",
    "    config_params = {\n",
    "        \"BATCH_SIZE\": 8, \"MAX_LEN\": 128, \"LEARNING_RATE\": 2e-5, \"EPOCHS\": 15,\n",
    "        \"NUM_PREDICTIONS\": 5, \"ORIGINAL_DATA_FILE\": \"/kaggle/input/phuc-data/comment_price.json\"\n",
    "    }\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Chia dữ liệu \n",
    "    with open(config_params[\"ORIGINAL_DATA_FILE\"], \"r\", encoding=\"utf-8\") as f:\n",
    "        full_data = json.load(f)\n",
    "    train_data, temp_data = train_test_split(full_data, test_size=0.2, random_state=42)\n",
    "    dev_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "    # Lưu vào thư mục gốc như code của bạn\n",
    "    json.dump(train_data, open(\"train.json\", \"w\")); json.dump(dev_data, open(\"dev.json\", \"w\")); json.dump(test_data, open(\"test.json\", \"w\"))\n",
    "\n",
    "    # Tạo tokenizer, dataset, dataloader \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"uitnlp/CafeBERT\", use_fast=True)\n",
    "    null_label_id = LABEL2ID.get(\"product#null#neutral\")\n",
    "    train_dataset = CommentDataset(\"train.json\", tokenizer, LABEL2ID, null_label_id, config_params[\"NUM_PREDICTIONS\"], config_params[\"MAX_LEN\"])\n",
    "    dev_dataset = CommentDataset(\"dev.json\", tokenizer, LABEL2ID, null_label_id, config_params[\"NUM_PREDICTIONS\"], config_params[\"MAX_LEN\"])\n",
    "    test_dataset = CommentDataset(\"test.json\", tokenizer, LABEL2ID, null_label_id, config_params[\"NUM_PREDICTIONS\"], config_params[\"MAX_LEN\"])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config_params[\"BATCH_SIZE\"], shuffle=True, num_workers=2)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=config_params[\"BATCH_SIZE\"], num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config_params[\"BATCH_SIZE\"], num_workers=2)\n",
    "\n",
    "    # Khởi tạo model và trainer \n",
    "    model = XLMRobertaCommentClassifier(num_classes=NUM_CLASSES, num_predictions=config_params[\"NUM_PREDICTIONS\"], lr=config_params[\"LEARNING_RATE\"])\n",
    "    checkpoint_callback = ModelCheckpoint(monitor='val_loss', dirpath='checkpoints', filename='best-model', save_top_k=1, mode='min')\n",
    "    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, mode='min')\n",
    "    \n",
    "    # Khai báo CSVLogger tường minh \n",
    "    csv_logger = CSVLogger(save_dir=\"logs/\", name=\"my_model_logs\")\n",
    "    \n",
    "    trainer = Trainer(max_epochs=config_params[\"EPOCHS\"], accelerator='auto', devices=1,\n",
    "                      callbacks=[checkpoint_callback, early_stopping_callback], \n",
    "                      logger=csv_logger, \n",
    "                      precision=\"16-mixed\", accumulate_grad_batches=2)\n",
    "\n",
    "    # Huấn luyện \n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e20b22",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
